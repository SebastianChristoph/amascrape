import asyncio
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timezone
import os
from typing import Dict, List, Optional

from app.auth import get_current_user
from app.database import get_db
from app.models import (Market, MarketChange, MarketCluster, Product,
                        ProductChange, User)
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from scraper.Product_Orchestrator import Product_Orchestrator  # ‚úÖ Richtig

from scraper.first_page_amazon_scraper import AmazonFirstPageScraper
from sqlalchemy.orm import Session
import logging


router = APIRouter()
executor = ThreadPoolExecutor()
orchestrator_task = None
orchestrator_running = False 


class NewClusterData(BaseModel):
    keywords: List[str]
    clusterName: Optional[str] = None


LOG_FILE = "scraping_log.txt"

# ‚úÖ Logging-Konfiguration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler()  # Direkt in die Konsole schreiben
    ]
)

scraping_processes: Dict[int, Dict[str, Dict[str, Dict[str, any]]]] = {}


@router.post("/start-firstpage-scraping-process")
async def post_scraping(
    newClusterData: NewClusterData,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Startet den Scraping-Prozess f√ºr einen Nutzer"""
    user_id = current_user.id
    cluster_name = newClusterData.clusterName
    print(f"üî• Start Scraping f√ºr Nutzer {user_id}, Cluster: {cluster_name}")

    scraping_processes[user_id] = {}
    scraping_processes[user_id][cluster_name] = {
        "status": "processing", "keywords": {}}

    for keyword in newClusterData.keywords:
        market_exists = db.query(Market).filter(
            Market.keyword == keyword).first()

        if market_exists:
            print(f"‚úÖ Market '{keyword}' existiert bereits.")
            scraping_processes[user_id][cluster_name]["keywords"][keyword] = {
                "status": "done", "data": {}}
        else:
            print(f"üîç Scraping f√ºr neues Keyword: {keyword}")
            scraping_processes[user_id][cluster_name]["keywords"][keyword] = {
                "status": "processing", "data": {}}
            asyncio.create_task(scraping_process(
                keyword, cluster_name, user_id))

    return {"success": True, "message": f"Scraping f√ºr {cluster_name} gestartet"}

@router.get("/get-loading-clusters")
async def get_loading_clusters(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Gibt den Status der laufenden Scraping-Prozesse zur√ºck"""
    user_id = current_user.id
    if user_id not in scraping_processes or not scraping_processes[user_id]:
        return {"active_clusters": []}

    clustername, cluster_data = next(iter(scraping_processes[user_id].items()))

    # ‚úÖ Pr√ºfen, ob Fehler vorliegen
    has_errors = any(
        data["status"] == "error" for data in cluster_data["keywords"].values())

    if has_errors:
        cluster_data["status"] = "error"
        active_cluster = {
            "clustername": clustername,
            "status": "error",
            "keywords": {kw: data["status"] for kw, data in cluster_data["keywords"].items()}
        }
        scraping_processes[user_id] = {}  # Reset Scraping-Prozess
        return active_cluster

    # ‚úÖ Pr√ºfen, ob alle Keywords fertig sind
    all_done = all(
        status["status"] == "done" for status in cluster_data["keywords"].values())

    if all_done:
        scraping_processes[user_id][clustername]["status"] = "done"
        print(
            f"‚úÖ Alle Keywords f√ºr '{clustername}' fertig! -> Datenbank schreiben")

        existing_cluster = db.query(MarketCluster).filter(
            MarketCluster.title == clustername, MarketCluster.user_id == user_id
        ).first()

        if not existing_cluster:
            new_cluster = MarketCluster(title=clustername, user_id=user_id)
            db.add(new_cluster)

            for keyword, keyword_data in scraping_processes[user_id][clustername]["keywords"].items():
                existing_market = db.query(Market).filter(
                    Market.keyword == keyword).first()

                if existing_market:
                    new_cluster.markets.append(existing_market)
                    continue

                new_market = Market(keyword=keyword)
                db.add(new_market)
                db.commit()
                db.refresh(new_market)
                new_cluster.markets.append(new_market)

                product_data_list = keyword_data["data"].get(
                    "first_page_products", [])
                top_suggestions = keyword_data["data"].get(
                    "top_search_suggestions", [])

                new_market_change = MarketChange(
                    market_id=new_market.id,
                    change_date=datetime.now(timezone.utc),
                    new_products=",".join(
                        [p["asin"] for p in product_data_list]) if product_data_list else "",
                )
                new_market_change.set_top_suggestions(top_suggestions)
                db.add(new_market_change)

                for product_data in product_data_list:
                    existing_product = db.query(Product).filter(
                        Product.asin == product_data["asin"]).first()

                    if not existing_product:
                        new_product = Product(asin=product_data["asin"])
                        db.add(new_product)
                        db.commit()
                        db.refresh(new_product)

                        new_market.products.append(new_product)
                        new_market_change.products.append(new_product)

                        new_product_change = ProductChange(
                            asin=new_product.asin,
                            title=product_data.get("title"),
                            price=product_data.get("price"),
                            main_category=product_data.get("main_category"),
                            second_category=product_data.get(
                                "second_category"),
                            main_category_rank=product_data.get(
                                "main_category_rank"),
                            second_category_rank=product_data.get(
                                "second_category_rank"),
                            img_path=product_data.get("image"),
                            change_date=datetime.now(timezone.utc),
                            changes="Initial creation",
                        )
                        db.add(new_product_change)
                        db.commit()
                        db.refresh(new_product_change)

                        new_product.product_changes.append(new_product_change)

            db.commit()
            db.refresh(new_cluster)

        del scraping_processes[user_id]
        return {"active_clusters": []}

    return {
        "clustername": clustername,
        "status": cluster_data["status"],
        "keywords": {kw: data["status"] for kw, data in cluster_data["keywords"].items()}
    }


async def scraping_process(keyword: str, clustername: str, user_id: int):
    """F√ºhrt das Scraping f√ºr ein Keyword durch"""
    print(f"‚è≥ Start Scraping ({keyword}) f√ºr Nutzer {user_id}")

    loop = asyncio.get_running_loop()
    first_page_data = await loop.run_in_executor(executor, fetch_first_page_data, keyword)

    if not first_page_data:
        first_page_data = {"first_page_products": [],
                           "top_search_suggestions": []}
        scraping_processes[user_id][clustername]["keywords"][keyword]["status"] = "error"
    else:
        scraping_processes[user_id][clustername]["keywords"][keyword]["status"] = "done"

    scraping_processes[user_id][clustername]["keywords"][keyword]["data"] = first_page_data
    print(f"‚úÖ Scraping f√ºr '{keyword}' abgeschlossen! (Nutzer {user_id})")


def fetch_first_page_data(keyword: str):
    """F√ºhrt das Scraping im Hintergrund aus"""
    amazon_scraper = AmazonFirstPageScraper(headless=True, show_details=True)
    return amazon_scraper.get_first_page_data(keyword)

@router.post("/start-product-orchestrator")
async def start_product_orchestrator(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Startet den Product Orchestrator asynchron."""
    global orchestrator_task, orchestrator_running

    # ‚úÖ Zugriff pr√ºfen
    if not is_admin(current_user):
        raise HTTPException(status_code=403, detail="Zugriff verweigert. Nur Admins d√ºrfen den Product Orchestrator starten.")

    if orchestrator_running:
        logging.warning("‚ö†Ô∏è Product Orchestrator l√§uft bereits! Kein zweiter Start m√∂glich.")
        return {"success": False, "message": "Product Orchestrator l√§uft bereits!"}

    # Falls bereits ein Prozess l√§uft, verhindere einen zweiten Start
    if orchestrator_task and not orchestrator_task.done():
        logging.warning("‚ö†Ô∏è Product Orchestrator l√§uft bereits!")
        return {"success": False, "message": "Product Orchestrator l√§uft bereits!"}

      # ‚úÖ Setzt den Status auf "l√§uft"
    orchestrator_running = True
    logging.info("üöÄ Starte Product Orchestrator...")

    loop = asyncio.get_running_loop()
    orchestrator_task = loop.run_in_executor(executor, run_product_orchestrator)

    logging.info("üéØ Orchestrator wurde erfolgreich in Thread gestartet.")
    
    return {"success": True, "message": "Product Orchestrator wurde gestartet!"}


def is_admin(user: User):
    """Pr√ºft, ob der eingeloggte Nutzer Admin ist."""
    return user.username == "admin"

def run_product_orchestrator():
    """Startet den Product Orchestrator."""
    global orchestrator_running
    try:
        logging.info("üöÄ Product Orchestrator wird gestartet...")
        
        # ‚úÖ Korrekte Instanziierung der Klasse
        orchestrator = Product_Orchestrator(just_scrape_3_products=False)  
        orchestrator.update_products()

        logging.info("‚úÖ Product Orchestrator abgeschlossen.")
    except Exception as e:
        logging.error(f"‚ùå Fehler im Product Orchestrator: {e}")
    finally:
        orchestrator_running = False  # ‚úÖ Status zur√ºcksetzen

@router.get("/is-product-orchestrator-running")
async def is_product_orchestrator_running(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Pr√ºft, ob der Product Orchestrator aktuell l√§uft. Nur f√ºr Admins!"""
    global orchestrator_running

    # ‚úÖ Zugriff pr√ºfen (nur Admins)
    if not is_admin(current_user):
        raise HTTPException(status_code=403, detail="Zugriff verweigert. Nur Admins d√ºrfen den Status abrufen.")

    return {"running": orchestrator_running}

@router.get("/get-product-orchestrator-logs")
async def get_orchestrator_logs(current_user: User = Depends(get_current_user)):
    """Gibt die letzten 20 Zeilen des Product Orchestrator Logs zur√ºck (Nur Admins)."""

    if current_user.username != "admin":
        raise HTTPException(status_code=403, detail="Zugriff verweigert. Nur Admins d√ºrfen Logs sehen.")

    if not os.path.exists(LOG_FILE):
        return {"logs": ["üö´ Keine Logs gefunden!"]}

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            logs = f.readlines()
        
        last_logs = logs[-20:]  # Nur die letzten 20 Zeilen senden
        return {"logs": last_logs}
    except Exception as e:
        return {"logs": [f"‚ùå Fehler beim Abrufen der Logs: {str(e)}"]}